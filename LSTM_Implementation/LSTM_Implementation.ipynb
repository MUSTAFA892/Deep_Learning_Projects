{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM-based Seq2Seq Chatbot using Cornell Movie Dialogue Corpus\n",
        "\n",
        "**Architecture:** Encoder-Decoder LSTM (Long Short-Term Memory)  \n",
        "**Model Type:** Sequence-to-Sequence (Seq2Seq)  \n",
        "**Dataset:** Cornell Movie-Dialogs Corpus  \n",
        "**Total Conversations:** 220,579 exchanges between 10,292 character pairs  \n",
        "**Total Utterances:** 304,713 from 617 movies  \n",
        "\n",
        "**Project Goals:**\n",
        "- Implement LSTM encoder-decoder architecture\n",
        "- Understand LSTM's memory mechanisms (forget, input, output gates)\n",
        "- Process large-scale conversational data\n",
        "- Train a generative dialogue model using LSTMs\n",
        "\n",
        "**Dataset Source:** https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n"
      ],
      "metadata": {
        "id": "JlEgkGh2jqyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install tensorflow numpy pandas matplotlib scikit-learn tqdm -q\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import re\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(\"Setup Complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LwntErpjwwS",
        "outputId": "a7269eb7-b605-4441-c757-9b7a9246f675"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "Setup Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Cornell Movie Dialogue Corpus\n",
        "\n",
        "The dataset contains:\n",
        "- `movie_lines.txt` - All utterances with character and movie metadata\n",
        "- `movie_conversations.txt` - Conversation structure showing dialogue flow\n",
        "- `movie_characters_metadata.txt` - Character information\n",
        "- `movie_titles_metadata.txt` - Movie details\n"
      ],
      "metadata": {
        "id": "kkOdp_dtkGMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Cornell Movie Dialogue Corpus\n",
        "!wget -q http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
        "!unzip -q cornell_movie_dialogs_corpus.zip\n",
        "\n",
        "# Verify download\n",
        "data_dir = 'cornell movie-dialogs corpus'\n",
        "if os.path.exists(data_dir):\n",
        "    print(\"✓ Dataset downloaded successfully!\")\n",
        "    print(\"\\nFiles in dataset:\")\n",
        "    for file in os.listdir(data_dir):\n",
        "        print(f\"  - {file}\")\n",
        "else:\n",
        "    print(\"✗ Download failed. Please check the URL.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7z50xFvkFx4",
        "outputId": "50e86640-e88b-4619-d82c-392b3b8c182c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dataset downloaded successfully!\n",
            "\n",
            "Files in dataset:\n",
            "  - README.txt\n",
            "  - movie_titles_metadata.txt\n",
            "  - .DS_Store\n",
            "  - movie_conversations.txt\n",
            "  - movie_lines.txt\n",
            "  - chameleons.pdf\n",
            "  - movie_characters_metadata.txt\n",
            "  - raw_script_urls.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Parsing the Dataset\n",
        "\n",
        "We need to:\n",
        "1. Load `movie_lines.txt` to create a line ID → text mapping\n",
        "2. Load `movie_conversations.txt` to get conversation sequences\n",
        "3. Extract question-answer pairs from consecutive utterances\n",
        "4. Clean and preprocess text data\n"
      ],
      "metadata": {
        "id": "Uu0Gzrh1kYYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_lines(file_path):\n",
        "    \"\"\"\n",
        "    Load movie lines into a dictionary\n",
        "    Format: lineID +++$+++ characterID +++$+++ movieID +++$+++ character +++$+++ text\n",
        "    \"\"\"\n",
        "    lines_dict = {}\n",
        "    with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            if len(parts) == 5:\n",
        "                line_id = parts[0]\n",
        "                text = parts[4].strip()\n",
        "                lines_dict[line_id] = text\n",
        "    return lines_dict\n",
        "\n",
        "# Load lines\n",
        "lines_file = os.path.join(data_dir, 'movie_lines.txt')\n",
        "id2line = load_lines(lines_file)\n",
        "\n",
        "print(f\"Total lines loaded: {len(id2line)}\")\n",
        "print(f\"\\nSample lines:\")\n",
        "for i, (line_id, text) in enumerate(list(id2line.items())[:5]):\n",
        "    print(f\"{line_id}: {text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQfds7t-kaaq",
        "outputId": "c21e39e9-1eef-4e21-a88a-3d79c1f502d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total lines loaded: 304713\n",
            "\n",
            "Sample lines:\n",
            "L1045: They do not!\n",
            "L1044: They do to!\n",
            "L985: I hope so.\n",
            "L984: She okay?\n",
            "L925: Let's go.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_conversations(file_path, id2line):\n",
        "    \"\"\"\n",
        "    Load conversations and create question-answer pairs\n",
        "    Format: characterID1 +++$+++ characterID2 +++$+++ movieID +++$+++ ['L1', 'L2', ...]\n",
        "    \"\"\"\n",
        "    conversations = []\n",
        "    with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            if len(parts) == 4:\n",
        "                # Extract line IDs from the conversation\n",
        "                line_ids = eval(parts[3])\n",
        "                conversations.append(line_ids)\n",
        "\n",
        "    # Create question-answer pairs\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations:\n",
        "        for i in range(len(conversation) - 1):\n",
        "            question_id = conversation[i]\n",
        "            answer_id = conversation[i + 1]\n",
        "\n",
        "            if question_id in id2line and answer_id in id2line:\n",
        "                question = id2line[question_id]\n",
        "                answer = id2line[answer_id]\n",
        "                qa_pairs.append([question, answer])\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# Load conversations\n",
        "conversations_file = os.path.join(data_dir, 'movie_conversations.txt')\n",
        "qa_pairs = load_conversations(conversations_file, id2line)\n",
        "\n",
        "print(f\"Total Q&A pairs created: {len(qa_pairs)}\")\n",
        "print(f\"\\nSample conversations:\")\n",
        "for i in range(5):\n",
        "    print(f\"Q: {qa_pairs[i][0]}\")\n",
        "    print(f\"A: {qa_pairs[i][1]}\")\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8s9ncsglF7S",
        "outputId": "0f529c64-99f5-4024-c93f-123100a4975e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Q&A pairs created: 221616\n",
            "\n",
            "Sample conversations:\n",
            "Q: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
            "A: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "--------------------------------------------------------------------------------\n",
            "Q: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "A: Not the hacking and gagging and spitting part.  Please.\n",
            "--------------------------------------------------------------------------------\n",
            "Q: Not the hacking and gagging and spitting part.  Please.\n",
            "A: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
            "--------------------------------------------------------------------------------\n",
            "Q: You're asking me out.  That's so cute. What's your name again?\n",
            "A: Forget it.\n",
            "--------------------------------------------------------------------------------\n",
            "Q: No, no, it's my fault -- we didn't have a proper introduction ---\n",
            "A: Cameron.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning and Preprocessing\n",
        "\n",
        "Steps:\n",
        "- Convert to lowercase\n",
        "- Remove special characters and extra spaces\n",
        "- Add start and end tokens for decoder\n",
        "- Filter extremely long or short sentences\n"
      ],
      "metadata": {
        "id": "On3w8bDVkivv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and normalize text\n",
        "    \"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters but keep basic punctuation\n",
        "    text = re.sub(r\"[^a-z0-9?.!,¿']+\", \" \", text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def filter_pairs(pairs, max_length=20):\n",
        "    \"\"\"\n",
        "    Filter pairs based on length\n",
        "    \"\"\"\n",
        "    filtered_pairs = []\n",
        "    for question, answer in pairs:\n",
        "        q_clean = clean_text(question)\n",
        "        a_clean = clean_text(answer)\n",
        "\n",
        "        # Filter by word count\n",
        "        if (len(q_clean.split()) <= max_length and\n",
        "            len(a_clean.split()) <= max_length and\n",
        "            len(q_clean.split()) > 0 and\n",
        "            len(a_clean.split()) > 0):\n",
        "            filtered_pairs.append([q_clean, a_clean])\n",
        "\n",
        "    return filtered_pairs\n",
        "\n",
        "# Clean and filter pairs\n",
        "print(\"Cleaning and filtering data...\")\n",
        "filtered_qa_pairs = filter_pairs(qa_pairs, max_length=15)\n",
        "\n",
        "# Limit to first 50,000 for faster training (you can increase this)\n",
        "filtered_qa_pairs = filtered_qa_pairs[:50000]\n",
        "\n",
        "print(f\"Filtered pairs: {len(filtered_qa_pairs)}\")\n",
        "print(f\"\\nSample cleaned conversations:\")\n",
        "for i in range(3):\n",
        "    print(f\"Q: {filtered_qa_pairs[i][0]}\")\n",
        "    print(f\"A: {filtered_qa_pairs[i][1]}\")\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EDZfMl9lMgx",
        "outputId": "f50e236b-75a8-4d7e-cb79-4adfa0ee971b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning and filtering data...\n",
            "Filtered pairs: 50000\n",
            "\n",
            "Sample cleaned conversations:\n",
            "Q: well, i thought we'd start with pronunciation, if that's okay with you.\n",
            "A: not the hacking and gagging and spitting part. please.\n",
            "--------------------------------------------------------------------------------\n",
            "Q: not the hacking and gagging and spitting part. please.\n",
            "A: okay... then how 'bout we try out some french cuisine. saturday? night?\n",
            "--------------------------------------------------------------------------------\n",
            "Q: you're asking me out. that's so cute. what's your name again?\n",
            "A: forget it.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate questions and answers\n",
        "questions = [pair[0] for pair in filtered_qa_pairs]\n",
        "answers = [pair[1] for pair in filtered_qa_pairs]\n",
        "\n",
        "# Add START and END tokens to answers\n",
        "answers_with_tags = ['<START> ' + answer + ' <END>' for answer in answers]\n",
        "\n",
        "print(f\"Total questions: {len(questions)}\")\n",
        "print(f\"Total answers: {len(answers_with_tags)}\")\n",
        "print(f\"\\nSample with tags:\")\n",
        "print(f\"Q: {questions[0]}\")\n",
        "print(f\"A: {answers_with_tags[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXfcCyQElRU9",
        "outputId": "0c742dc5-28b2-41e9-8d4f-17f7eee792e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total questions: 50000\n",
            "Total answers: 50000\n",
            "\n",
            "Sample with tags:\n",
            "Q: well, i thought we'd start with pronunciation, if that's okay with you.\n",
            "A: <START> not the hacking and gagging and spitting part. please. <END>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and Sequence Creation\n",
        "\n",
        "Convert text to sequences of integers for RNN processing:\n",
        "- Build vocabulary from all words\n",
        "- Convert sentences to integer sequences\n",
        "- Pad sequences to uniform length\n"
      ],
      "metadata": {
        "id": "zIi6hAMKlT5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tokenizer\n",
        "tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(questions + answers_with_tags)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Convert to sequences\n",
        "question_sequences = tokenizer.texts_to_sequences(questions)\n",
        "answer_sequences = tokenizer.texts_to_sequences(answers_with_tags)\n",
        "\n",
        "# Find max length\n",
        "max_question_len = max([len(seq) for seq in question_sequences])\n",
        "max_answer_len = max([len(seq) for seq in answer_sequences])\n",
        "\n",
        "print(f\"Max Question Length: {max_question_len}\")\n",
        "print(f\"Max Answer Length: {max_answer_len}\")\n",
        "\n",
        "# Pad sequences\n",
        "encoder_input = pad_sequences(question_sequences, maxlen=max_question_len, padding='post')\n",
        "decoder_input = pad_sequences(answer_sequences, maxlen=max_answer_len, padding='post')\n",
        "\n",
        "# Create decoder output (shifted by one position)\n",
        "decoder_output = []\n",
        "for seq in answer_sequences:\n",
        "    decoder_output.append(seq[1:])  # Remove <START> token\n",
        "\n",
        "decoder_output = pad_sequences(decoder_output, maxlen=max_answer_len, padding='post')\n",
        "\n",
        "print(f\"\\nEncoder Input Shape: {encoder_input.shape}\")\n",
        "print(f\"Decoder Input Shape: {decoder_input.shape}\")\n",
        "print(f\"Decoder Output Shape: {decoder_output.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkJhms_U-geD",
        "outputId": "990aaae4-12dc-48f7-bc60-e84c5715b72d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 38743\n",
            "Max Question Length: 15\n",
            "Max Answer Length: 17\n",
            "\n",
            "Encoder Input Shape: (50000, 15)\n",
            "Decoder Input Shape: (50000, 17)\n",
            "Decoder Output Shape: (50000, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq LSTM Architecture\n",
        "\n",
        "**Encoder-Decoder LSTM Architecture:**\n",
        "- **Encoder LSTM**: Processes question sequence, outputs context vectors (hidden state + cell state)\n",
        "- **Decoder LSTM**: Takes context + previous word, generates next word using LSTM cells\n",
        "- **LSTM Advantages**:\n",
        "  - Three gates (forget, input, output) for selective memory\n",
        "  - Cell state for long-term dependencies\n",
        "  - Better gradient flow than vanilla RNN\n",
        "\n",
        "This is a classic **Encoder-Decoder LSTM** model for sequence-to-sequence dialogue generation.\n"
      ],
      "metadata": {
        "id": "1qUCqpKi-sPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "embedding_dim = 128\n",
        "lstm_units = 256\n",
        "batch_size = 64\n",
        "epochs = 3\n",
        "\n",
        "# Encoder\n",
        "encoder_input_layer = Input(shape=(max_question_len,))\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)(encoder_input_layer)\n",
        "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_input_layer = Input(shape=(max_answer_len,))\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)(decoder_input_layer)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Complete model\n",
        "model = Model([encoder_input_layer, decoder_input_layer], decoder_outputs)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "oQ9eDNDU-u_1",
        "outputId": "1112a868-f321-442c-f487-5bff8cc0729e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m4,959,104\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m4,959,104\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m394,240\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m394,240\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m38743\u001b[0m) │  \u001b[38;5;34m9,956,951\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,959,104</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,959,104</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38743</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">9,956,951</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,663,639\u001b[0m (78.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,663,639</span> (78.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,663,639\u001b[0m (78.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,663,639</span> (78.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Chatbot\n",
        "\n",
        "This will take some time depending on your dataset size and hardware.\n",
        "\n",
        "**Training Process:**\n",
        "- Encoder processes the question\n",
        "- Decoder learns to generate the answer word-by-word\n",
        "- Loss function: Sparse categorical crossentropy (predicting next word)\n"
      ],
      "metadata": {
        "id": "wnuMkS2q-0nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare decoder output for training (add dimension for sparse_categorical_crossentropy)\n",
        "decoder_output_train = decoder_output.reshape(decoder_output.shape[0], decoder_output.shape[1], 1)\n",
        "\n",
        "# Train\n",
        "print(\"Starting training...\")\n",
        "history = model.fit(\n",
        "    [encoder_input, decoder_input],\n",
        "    decoder_output_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Training Complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T1BW0Hg-2-e",
        "outputId": "52b6369d-d86d-4546-d937-2528c0707326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/3\n",
            "\u001b[1m467/704\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m25:23\u001b[0m 6s/step - accuracy: 0.2242 - loss: 6.8696"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "plt.title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "plt.title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hu8tGfaS-9vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Models for Response Generation\n",
        "\n",
        "For chatbot interaction, we need separate encoder and decoder models:\n",
        "- **Encoder Model**: Processes user input, outputs context states\n",
        "- **Decoder Model**: Generates response word-by-word using context\n"
      ],
      "metadata": {
        "id": "q4EMJXRI_Ozj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input_layer, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(lstm_units,))\n",
        "decoder_state_input_c = Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_embedding_inf = decoder_embedding\n",
        "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(\n",
        "    decoder_embedding_inf, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states_inf = [state_h_inf, state_c_inf]\n",
        "decoder_outputs_inf = decoder_dense(decoder_outputs_inf)\n",
        "\n",
        "decoder_model_inf = Model(\n",
        "    [decoder_input_layer] + decoder_states_inputs,\n",
        "    [decoder_outputs_inf] + decoder_states_inf\n",
        ")\n",
        "\n",
        "print(\"✓ Inference models created successfully!\")\n"
      ],
      "metadata": {
        "id": "5-ZpFqbS_Od9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Chatbot Responses\n",
        "\n",
        "The generation process:\n",
        "1. Encode user input to get context states\n",
        "2. Initialize decoder with <START> token\n",
        "3. Predict next word iteratively until <END> or max length\n",
        "4. Return generated response\n"
      ],
      "metadata": {
        "id": "VFFAsJwQ_TZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(input_text):\n",
        "    \"\"\"\n",
        "    Generate chatbot response for given input\n",
        "    \"\"\"\n",
        "    # Clean input\n",
        "    input_text = clean_text(input_text)\n",
        "\n",
        "    # Convert to sequence\n",
        "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_question_len, padding='post')\n",
        "\n",
        "    # Encode input\n",
        "    states_value = encoder_model_inf.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Generate empty target sequence\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer.word_index.get('<START>', 1)\n",
        "\n",
        "    # Generate response word by word\n",
        "    decoded_sentence = []\n",
        "    stop_condition = False\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model_inf.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # Sample next word\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = None\n",
        "\n",
        "        # Find word from index\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == sampled_token_index:\n",
        "                sampled_word = word\n",
        "                break\n",
        "\n",
        "        # Exit conditions\n",
        "        if sampled_word == '<END>' or len(decoded_sentence) > max_answer_len:\n",
        "            stop_condition = True\n",
        "        elif sampled_word and sampled_word not in ['<START>', '<OOV>']:\n",
        "            decoded_sentence.append(sampled_word)\n",
        "\n",
        "        # Update target sequence\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return ' '.join(decoded_sentence)\n",
        "\n",
        "print(\"✓ Response generator ready!\")\n"
      ],
      "metadata": {
        "id": "dPr2_wYl_UWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with sample inputs\n",
        "test_questions = [\n",
        "    \"hi\",\n",
        "    \"how are you\",\n",
        "    \"what is your name\",\n",
        "    \"where are you from\",\n",
        "    \"tell me about yourself\",\n",
        "    \"goodbye\",\n",
        "    \"i love you\",\n",
        "    \"what do you think\",\n",
        "    \"can you help me\",\n",
        "    \"thank you\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TESTING CHATBOT RESPONSES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for question in test_questions:\n",
        "    response = generate_response(question)\n",
        "    print(f\"\\nYou: {question}\")\n",
        "    print(f\"Bot: {response}\")\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "oAfjS264_iss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Chatbot Session\n",
        "\n",
        "Now you can have a real conversation with your RNN chatbot!\n",
        "\n",
        "**Note:** The chatbot's responses depend on training quality and may sometimes be:\n",
        "- Repetitive (due to RNN's short-term memory)\n",
        "- Generic (learned from movie dialogues)\n",
        "- Creative (unexpected combinations from training data)\n",
        "\n",
        "Type 'quit', 'exit', or 'bye' to end the conversation.\n"
      ],
      "metadata": {
        "id": "ijk7Wq6A_mdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_interactive():\n",
        "    \"\"\"\n",
        "    Interactive chatbot interface\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"🤖 CORNELL MOVIE DIALOGUE RNN CHATBOT\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Start chatting! (Type 'quit', 'exit', or 'bye' to end)\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit', 'bye', 'goodbye']:\n",
        "            response = generate_response(user_input)\n",
        "            print(f\"Bot: {response}\")\n",
        "            print(\"\\n👋 Thanks for chatting! Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if user_input.strip() == '':\n",
        "            continue\n",
        "\n",
        "        response = generate_response(user_input)\n",
        "        print(f\"Bot: {response}\\n\")\n",
        "\n",
        "# Start interactive chat\n",
        "chat_interactive()\n"
      ],
      "metadata": {
        "id": "v2OpTXcO_qPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Your Trained Chatbot\n",
        "\n",
        "Save your model and tokenizer for future use without retraining.\n"
      ],
      "metadata": {
        "id": "pqKgQw_U_uE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save models\n",
        "model.save('rnn_chatbot_full_model.h5')\n",
        "encoder_model_inf.save('encoder_model.h5')\n",
        "decoder_model_inf.save('decoder_model.h5')\n",
        "\n",
        "# Save tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# Save configuration\n",
        "config = {\n",
        "    'max_question_len': max_question_len,\n",
        "    'max_answer_len': max_answer_len,\n",
        "    'vocab_size': vocab_size,\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'lstm_units': lstm_units\n",
        "}\n",
        "\n",
        "with open('model_config.pickle', 'wb') as f:\n",
        "    pickle.dump(config, f)\n",
        "\n",
        "print(\"✓ Models and configuration saved successfully!\")\n",
        "print(\"\\nSaved files:\")\n",
        "print(\"  - rnn_chatbot_full_model.h5\")\n",
        "print(\"  - encoder_model.h5\")\n",
        "print(\"  - decoder_model.h5\")\n",
        "print(\"  - tokenizer.pickle\")\n",
        "print(\"  - model_config.pickle\")\n"
      ],
      "metadata": {
        "id": "Ut6Bdsoi_yg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Memory Mechanisms and Limitations\n",
        "\n",
        "**How LSTM Solves RNN's Vanishing Gradient Problem:**\n",
        "\n",
        "1. **Gating Mechanisms**: LSTM uses three gates (forget, input, output) to control information flow\n",
        "2. **Cell State**: Maintains a separate memory pathway that allows gradients to flow more smoothly\n",
        "3. **Additive Updates**: Cell state uses addition (not multiplication), preventing gradient decay\n",
        "4. **Selective Memory**: Network learns *when* to remember and *when* to forget information\n",
        "\n",
        "**Why LSTM is Better Than Vanilla RNN:**\n",
        "\n",
        "- ✅ Handles sequences up to 100+ tokens (vs RNN's ~10-15 tokens)\n",
        "- ✅ Prevents vanishing gradients through constant error flow\n",
        "- ✅ Better at capturing long-term dependencies in conversations\n",
        "- ✅ More stable training with deeper networks\n",
        "\n",
        "**However, LSTM Still Has Limitations:**\n",
        "\n",
        "1. **Sequential Processing**: Cannot parallelize like Transformers - slower training\n",
        "2. **Computational Cost**: 4x more parameters than vanilla RNN (3 gates + cell state)\n",
        "3. **Very Long Sequences**: Still struggles with sequences >200 tokens\n",
        "4. **Context Window**: Limited memory compared to attention-based models\n",
        "5. **Training Time**: Requires more epochs and computational resources\n",
        "\n",
        "**Performance Comparison:**\n",
        "\n",
        "| Model | Max Sequence Length | Parallelization | Training Speed | Long-term Memory |\n",
        "|-------|-------------------|-----------------|----------------|------------------|\n",
        "| RNN | ~10-15 tokens | ❌ No | Fast | ❌ Poor |\n",
        "| **LSTM (This Model)** | ~100-200 tokens | ❌ No | Moderate | ✅ Good |\n",
        "| GRU | ~100-200 tokens | ❌ No | Faster than LSTM | ✅ Good |\n",
        "| Transformer (BERT, GPT) | 512-4096+ tokens | ✅ Yes | Very Fast | ✅✅ Excellent |\n",
        "\n",
        "**When to Use LSTM:**\n",
        "\n",
        "- ✅ Small to medium datasets (LSTM often outperforms Transformers on small data)\n",
        "- ✅ Real-time applications with limited computational resources\n",
        "- ✅ Sequential tasks with moderate-length dependencies (chatbots, sentiment analysis)\n",
        "- ✅ Time-series prediction and forecasting\n",
        "\n",
        "**Next Steps - Modern Alternatives:**\n",
        "\n",
        "- **GRU (Gated Recurrent Unit)**: Simplified LSTM with fewer parameters, often similar performance\n",
        "- **Attention Mechanisms**: Add attention layers to LSTM for better context focus\n",
        "- **Transformers**: State-of-the-art for NLP (BERT, GPT, T5) - use attention instead of recurrence\n",
        "- **Hybrid Models**: Combine LSTM with Transformers for best of both worlds\n",
        "\n",
        "**This chatbot demonstrates LSTM's strong sequential learning capabilities. For production chatbots with massive datasets, consider Transformer-based models (GPT, DialoGPT, BlenderBot)!**\n"
      ],
      "metadata": {
        "id": "TqdqV2dfArF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze model behavior with different input lengths\n",
        "test_lengths = {\n",
        "    \"Short\": \"hi there\",\n",
        "    \"Medium\": \"how are you doing today\",\n",
        "    \"Long\": \"can you tell me what you think about the weather today and tomorrow\"\n",
        "}\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ANALYZING RNN BEHAVIOR WITH DIFFERENT INPUT LENGTHS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for length_type, test_input in test_lengths.items():\n",
        "    response = generate_response(test_input)\n",
        "    word_count = len(test_input.split())\n",
        "    print(f\"\\n{length_type} Input ({word_count} words):\")\n",
        "    print(f\"Input: {test_input}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "QpiiH1Pk_81C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}